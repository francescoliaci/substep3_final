# ============================================================
# Substep 3 â€“ Task-Graph Encoding + Step Matching (UPDATED)
# Works with your Substep-1 output_subtask_1.npy dict:
#   key   = "<video_id>_<start>_<end>"   e.g. "10_16_360p_0.00_4.80"
#   value = np.ndarray shape (1792,)
#
# What it does:
# 1) Loads the dict of pseudo-step embeddings (sliding windows)
# 2) Groups them per video_id
# 3) Encodes task-graph node texts using EgoVLP text encoder (1792-D)
# 4) Visual step embeddings are already 1792-D (EgoVLP-aligned) 
# 5) Runs Hungarian matching (1-to-1) between visual steps and graph nodes
# 6) Fuses matched node features with their matched visual features
# 7) Saves a realized graph per video as .npz (ready for Substep 4)
# ============================================================
import os
import sys
import json
from collections import defaultdict

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F

from scipy.optimize import linear_sum_assignment
from tqdm import tqdm

# ------------------------------------------------------------
# PATHS (EDIT THESE TO YOUR SETUP)
# ------------------------------------------------------------

FINAL_FEATURES_PATH = "/content/drive/MyDrive/AMLproject/output_subtask_1.npy"
GRAPH_DIR           = "/content/drive/MyDrive/AMLproject/task_graphs"
CSV_PATH            = "/content/drive/MyDrive/AMLproject/activity_idx_step_idx.csv"
OUTPUT_DIR          = "/content/drive/MyDrive/AMLproject/graph_realizations_2"
EGOVLP_CKPT         = "/content/drive/MyDrive/AMLproject/pretrained/EgoVLP_PT_BEST.pth"
TEXT_EMB_DIR        = "/content/drive/MyDrive/AMLproject/text_embeddings"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
os.makedirs(OUTPUT_DIR, exist_ok=True)

VISUAL_DIM = 1792
TEXT_DIM = 256

visual_proj = nn.Linear(VISUAL_DIM, TEXT_DIM).to(DEVICE)
visual_proj.eval()  # training optional for now

# ------------------------------------------------------------
# BASIC SANITY CHECKS (HELPFUL WHEN RUNNING ON COLAB)
# ------------------------------------------------------------

print("DEVICE:", DEVICE)
print("FINAL_FEATURES_PATH exists:", os.path.exists(FINAL_FEATURES_PATH))
print("GRAPH_DIR exists:", os.path.exists(GRAPH_DIR))
print("CSV_PATH exists:", os.path.exists(CSV_PATH))
print("OUTPUT_DIR:", OUTPUT_DIR)

# ------------------------------------------------------------
# LOAD CSV MAPPING: activity_idx -> activity_name
# ------------------------------------------------------------

df = pd.read_csv(CSV_PATH)

id_to_recipe = (
    df[["activity_idx", "activity_name"]]
    .drop_duplicates()
    .set_index("activity_idx")["activity_name"]
    .to_dict()
)

# ------------------------------------------------------------
# NODE FUSION MODULE
# ------------------------------------------------------------

class NodeFusion(nn.Module):
    def __init__(self, dim=256):
        super().__init__()
        self.proj = nn.Linear(2 * dim, dim)

    def forward(self, text_feat, visual_feat):
        return self.proj(torch.cat([text_feat, visual_feat], dim=-1))

fusion = NodeFusion(256).to(DEVICE)
fusion.eval()

# ------------------------------------------------------------
# LOAD SUBSTEP 1 OUTPUT (DICT OF WINDOW FEATURES)
# ------------------------------------------------------------

print("\nLoading Substep-1 dict from:", FINAL_FEATURES_PATH)
data = np.load(FINAL_FEATURES_PATH, allow_pickle=True).item()

print("Type:", type(data))
print("Number of segments:", len(data))

# Peek at a few items
for i, (k, v) in enumerate(data.items()):
    print("Example key:", k, "shape:", np.array(v).shape)
    if i == 2:
        break

# ------------------------------------------------------------
# GROUP FEATURES BY VIDEO
# Your keys are like: "10_16_360p_0.00_4.80"
# We must extract video_id = "10_16_360p" (everything except last 2 parts).
# ------------------------------------------------------------

video_to_steps = defaultdict(list)
video_to_times = defaultdict(list)  # store (start, end) for optional debugging

for key, feat in data.items():
    parts = key.split("_")
    if len(parts) < 3:
        # Not enough parts to parse <video>_<start>_<end>
        continue

    # video_id = all parts except last two (start, end)
    video_id = "_".join(parts[:-2])

    # parse start/end if possible (optional)
    try:
        start_t = float(parts[-2])
        end_t   = float(parts[-1])
    except:
        start_t, end_t = None, None

    video_to_steps[video_id].append(feat)
    video_to_times[video_id].append((start_t, end_t))

print("\nðŸŽ¥ Videos found:", len(video_to_steps))
print("Example video_id:", next(iter(video_to_steps.keys())))

# ------------------------------------------------------------
# MAIN LOOP: MATCH STEPS TO TASK GRAPH
# ------------------------------------------------------------

# Optional counters for logging
saved_count = 0
skipped_no_graph = 0
skipped_no_recipe = 0
skipped_empty = 0

for video_id, step_feats in tqdm(video_to_steps.items(), desc="Matching videos"):

    if len(step_feats) == 0:
        skipped_empty += 1
        continue

    # --------------------------------------------------------
    # MAP VIDEO -> RECIPE ID
    # Your video_id begins with activity_idx (e.g., "10_16_360p")
    # so recipe_id = int(video_id.split("_")[0]) should be correct.
    # --------------------------------------------------------
    try:
        recipe_id = int(video_id.split("_")[0])
    except:
        skipped_no_recipe += 1
        continue

    if recipe_id not in id_to_recipe:
        skipped_no_recipe += 1
        continue

    recipe_name = id_to_recipe[recipe_id]

    # Your graphs are stored as <recipe_name_without_spaces>.json
    # If your graph filenames differ, adjust this.
    safe_name = recipe_name.lower().replace(" ", "")
    graph_path = os.path.join(GRAPH_DIR, f"{safe_name}.json")

    if not os.path.exists(graph_path):
        skipped_no_graph += 1
        continue

    # --------------------------------------------------------
    # LOAD TASK GRAPH JSON
    # Expected structure:
    #   graph["steps"] : dict { node_id(str/int) : text(str) }
    #   graph["edges"] : list of edges
    # --------------------------------------------------------
    with open(graph_path, "r") as f:
        graph = json.load(f)

    node_ids   = sorted(graph["steps"].keys(), key=int)
    node_texts = [graph["steps"][i] for i in node_ids]

    # --------------------------------------------------------
    # MAP ORIGINAL NODE IDS -> COMPACT INDICES (0..N-1)
    # --------------------------------------------------------

    node_ids_int = [int(i) for i in node_ids]

    id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids_int)}
    idx_to_id = {idx: node_id for idx, node_id in enumerate(node_ids_int)}
    # remap edges
    edges_remapped = [
        [id_to_idx[int(u)], id_to_idx[int(v)]]
        for u, v in graph["edges"]
    ]
    # sanity check
    num_nodes = len(node_ids_int)
    if len(edges_remapped) > 0:
        m = np.max(edges_remapped)
        if m >= num_nodes:
            raise ValueError(f"Bad remap: max edge idx {m} >= num_nodes {num_nodes} for {video_id}")

    # --------------------------------------------------------
    # TEXT ENCODING (TASK GRAPH NODES) -> (num_nodes, 256)
    # --------------------------------------------------------
    node_emb_path = os.path.join(
        TEXT_EMB_DIR,
        f"{recipe_name}_node_embeddings.npy"
    )

    meta_path = os.path.join(
        TEXT_EMB_DIR,
        f"{recipe_name}_meta.json"
    )

    if not os.path.exists(node_emb_path):
        skipped_no_graph += 1
        continue

    T = torch.from_numpy(np.load(node_emb_path)).float().to(DEVICE)  # (N, 256)
    T = F.normalize(T, dim=1)

    # --------------------------------------------------------
    # VISUAL STEP EMBEDDINGS
    # step_feats is a list of (1792,) numpy arrays.
    # We stack -> (num_steps, 1792)
    # Normalize -> cosine similarity space
    # --------------------------------------------------------
    V_np = np.stack(step_feats).astype(np.float32)  # (S, VISUAL_DIM)

    V = torch.tensor(step_feats).float().to(DEVICE)   # (S, 1792)
    V = visual_proj(V)                                # (S, 256)
    V = F.normalize(V, dim=1)

    # --------------------------------------------------------
    # TEMPORALLY + TOPOLOGICALLY CONSTRAINED HUNGARIAN MATCHING
    # --------------------------------------------------------

    # ---- 1) reorder visual steps by time ----
    times = np.array(video_to_times[video_id], dtype=object)
    starts = np.array([t[0] for t in times], dtype=np.float32)

    # --------------------------------------------------------
    # TEMPORAL CLUSTERING OF SLIDING WINDOWS â†’ PSEUDO-STEPS
    # --------------------------------------------------------

    # ensure temporal order
    order = np.argsort(starts)
    V_sorted = V[order]
    times_sorted = times[order]

    clusters = []
    current_cluster = [0]

    # cosine similarity threshold (EgoVLP works well with 0.85â€“0.9)
    CLUSTER_SIM_THRESHOLD = 0.94

    for i in range(1, V_sorted.shape[0]):
        sim = torch.dot(V_sorted[i - 1], V_sorted[i]).item()
        if sim >= CLUSTER_SIM_THRESHOLD:
            current_cluster.append(i)
        else:
            clusters.append(current_cluster)
            current_cluster = [i]

    clusters.append(current_cluster)

    # --------------------------------------------------------
    # FORCE REASONABLE NUMBER OF PSEUDO-STEPS
    # --------------------------------------------------------

    MAX_RATIO = 2.0  # allow at most 2Ã— the number of graph nodes

    while len(clusters) > MAX_RATIO * num_nodes:
        merged = []
        i = 0
        while i < len(clusters):
            if i < len(clusters) - 1:
                merged.append(clusters[i] + clusters[i + 1])
                i += 2
            else:
                merged.append(clusters[i])
                i += 1
        clusters = merged

    # build pseudo-steps
    pseudo_feats = []
    pseudo_times = []

    for c in clusters:
        feats = V_sorted[c]
        pseudo_feats.append(feats.mean(dim=0))

        start_t = times_sorted[c[0]][0]
        end_t   = times_sorted[c[-1]][1]
        pseudo_times.append((start_t, end_t))

    # V is now (K, 256) â€” projected visual pseudo-steps
    V = torch.stack(pseudo_feats, dim=0) # (K, 256)        
    V = F.normalize(V, dim=1)
    times = np.array(pseudo_times, dtype=object)

    S = V.shape[0]

    print(
        f"[INFO] Video {video_id}: "
        f"{len(step_feats)} windows â†’ {S} pseudo-steps "
        f"(graph nodes: {num_nodes})"
    )

    starts = np.array([t[0] for t in times], dtype=np.float32)
    order_steps = np.argsort(starts)
    V_ord = V[order_steps]          # (S, 1792)

    # ---- 2) build DAG and get topological order ----
    G = nx.DiGraph()
    G.add_nodes_from(range(num_nodes))
    G.add_edges_from(edges_remapped)

    node_order = list(nx.topological_sort(G))
    T_ord = T[node_order]           # (N, 1792)

    # ---- 3) similarity + base cost ----
    sim  = V_ord @ T_ord.T
    cost = -sim.detach().cpu().numpy()

    # ---- 4) diagonal (time/order) penalty ----
    S, N = cost.shape
    ii = np.arange(S)[:, None] / max(S - 1, 1)
    jj = np.arange(N)[None, :] / max(N - 1, 1)

    lambda_time = 3.0   
    time_penalty = (ii - jj) ** 2

    cost = cost + lambda_time * time_penalty

    # ---- 5) Hungarian ----
    row_ind, col_ind = linear_sum_assignment(cost)

    # ---- 6) map indices back to ORIGINAL indexing ----
    row_ind = order_steps[row_ind]
    col_ind = np.array([node_order[c] for c in col_ind], dtype=np.int32)

    matched_pairs = np.stack([row_ind, col_ind], axis=1)

    pairs_sorted = matched_pairs[np.argsort(matched_pairs[:, 0])]
    node_seq = pairs_sorted[:, 1]
    violations = np.sum(np.diff(node_seq) < 0)

    print(f"[DEBUG] Order violations: {violations} / {len(node_seq)}")


    # --------------------------------------------------------
    # FUSE MATCHED NODES
    # Start from node_features = T (text-only)
    # For matched node c, replace with fusion(text_node, matched_visual_step)
    # --------------------------------------------------------
    node_features = T.clone()

    for r, c in matched_pairs:
        node_features[c] = fusion(T[c], V[r])

    # --------------------------------------------------------
    # SAVE GRAPH REALIZATION PER VIDEO
    # --------------------------------------------------------
    out_path = os.path.join(OUTPUT_DIR, f"{video_id}_graph.npz")

    np.savez(
        out_path,
        # node features aligned with indices 0..N-1
        node_features=node_features.detach().cpu().numpy(),  # (N, 256)
        # edges now use compact indices
        edges=np.array(edges_remapped, dtype=np.int32),
        # text aligned with node_features
        node_texts=np.array(node_texts, dtype=object),
        # hungarian output already uses compact indices (OK)
        matched_pairs=matched_pairs,
        # optional but useful
        step_times=times,
        # DEBUG 
        original_node_ids=np.array(node_ids_int, dtype=np.int32)
    )   

    saved_count += 1

print("\nâœ… Substep 3 completed.")
print("Saved graphs:", saved_count)
print("Skipped (no recipe in CSV):", skipped_no_recipe)
print("Skipped (no graph json found):", skipped_no_graph)
print("Skipped (empty):", skipped_empty)

# ------------------------------------------------------------
# QUICK SANITY CHECK: LOAD ONE SAVED FILE AND PRINT SUMMARY
# ------------------------------------------------------------
saved_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith("_graph.npz")]
print("\nFiles saved in OUTPUT_DIR:", len(saved_files))

if len(saved_files) > 0:
    example = saved_files[0]
    x = np.load(os.path.join(OUTPUT_DIR, example), allow_pickle=True)

    print("\n--- Example realized graph:", example, "---")
    print("node_features shape:", x["node_features"].shape)
    print("edges shape:", x["edges"].shape)
    print("num node_texts:", len(x["node_texts"]))
    print("matched_pairs shape:", x["matched_pairs"].shape)
    print("first 5 matched pairs (visual_step_idx, node_idx):", x["matched_pairs"][:5])
